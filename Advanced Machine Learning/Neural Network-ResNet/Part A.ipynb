{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Improving CIFAR-10 Recognition Accuracy with State-of-the-art CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPljXbApsjdjPS68w8xt7bt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tj7I8t17l9zX"},"source":["Mount the Drive"]},{"cell_type":"code","metadata":{"id":"DymU6no0lIDW","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603006384866,"user_tz":-540,"elapsed":822,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"0c6817b3-87be-4b24-ba66-618f5e572dea"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i0Khkl2EmEVO"},"source":["### Useful imports and plotting functions"]},{"cell_type":"code","metadata":{"id":"fPVA7STOlZ1W"},"source":["%matplotlib inline\n","\n","# Python 2/3 compatibility\n","from __future__ import print_function, division\n","\n","import itertools\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Colors from Colorbrewer Paired_12\n","colors = [[31, 120, 180], [51, 160, 44]]\n","colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n","\n","# functions to show an image\n","def imshow(img):\n","    \"\"\"\n","    :param img: (PyTorch Tensor)\n","    \"\"\"\n","    # unnormalize\n","    img = img / 2 + 0.5     \n","    # Convert tensor to numpy array\n","    npimg = img.numpy()\n","    # Color channel first -> color channel last\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","def plot_losses(train_history, val_history):\n","    x = np.arange(1, len(train_history) + 1)\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n","    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend(loc='upper right')\n","    plt.title(\"Evolution of the training and validation loss\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v49oyEaBlc1R"},"source":["import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yaUY-_mfwGkT"},"source":["## PART A. Improving CIFAR-10 Recognition Accuracy with State-of-the-art CNN"]},{"cell_type":"markdown","metadata":{"id":"F6N41rTFmHU_"},"source":["### 1. Loading and normalizing CIFAR10 Dataset"]},{"cell_type":"markdown","metadata":{"id":"ttblK-ZSmM0N"},"source":["Seed the random generator to have reproducible results:\n"]},{"cell_type":"code","metadata":{"id":"KQpSKQ_zlfLV"},"source":["seed = 5678\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","if torch.cuda.is_available():\n","  # Make CuDNN Determinist\n","  torch.backends.cudnn.deterministic = True\n","  torch.cuda.manual_seed(seed)\n","\n","# Define default device, we should use the GPU (cuda) if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DVFs6a-amSrY"},"source":["Define subset of the dataset (so it is faster to train)"]},{"cell_type":"code","metadata":{"id":"x1cDkNktlhih"},"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","\n","n_training_samples = 50000 # Max: 50 000 - n_val_samples\n","n_val_samples = 0\n","n_test_samples = 10000\n","\n","train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n","val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n","test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n","# (In the last case, indexes do not need to account for training ones because the train=False parameter in datasets.CIFAR will select from the test set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hkBkVfY5mZrv"},"source":["Data augmentation and Normalization"]},{"cell_type":"code","metadata":{"id":"46Ova8DLlkQv","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1603006388233,"user_tz":-540,"elapsed":4143,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"20886a7f-fb25-4308-dbdb-d880f5256dc7"},"source":["num_workers = 2\n","test_batch_size = 4\n","\n","transform = transforms.Compose(\n","    [transforms.RandomHorizontalFlip(),\n","     transforms.RandomCrop(size=32,padding=4),\n","     transforms.ToTensor(),\n","     transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","\n","train_set = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks', train=True,\n","                                        download=True, transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch_size, sampler=train_sampler,\n","                                          num_workers=num_workers)\n","\n","test_set = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks', train=False,\n","                                       download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size, sampler=test_sampler,\n","                                         num_workers=num_workers)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WOkjJLNPlGeH"},"source":["Numbers of training, validation, testing samples"]},{"cell_type":"code","metadata":{"id":"LiqvWkeXOgR_","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603006388234,"user_tz":-540,"elapsed":4131,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"5429d5b2-1180-40fd-eff5-0d70e670f59a"},"source":["print(len(train_set))\n","print(len(val_sampler))\n","print(len(test_set))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["50000\n","0\n","10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"clCgIhdImkg-"},"source":["### 2.Network Setting - ResNet"]},{"cell_type":"code","metadata":{"id":"SA9FgfH-lmct","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603006388235,"user_tz":-540,"elapsed":4120,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"257e4e50-3952-4929-92cf-78303fe51ff0"},"source":["def _weights_init(m):\n","    classname = m.__class__.__name__\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight)\n","\n","class LambdaLayer(nn.Module):\n","    def __init__(self, lambd):\n","        super(LambdaLayer, self).__init__()\n","        self.lambd = lambd\n","\n","    def forward(self, x):\n","        return self.lambd(x)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, option='A'):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            if option == 'A':\n","                \"\"\"\n","                For CIFAR10 ResNet paper uses option A.\n","                \"\"\"\n","                self.shortcut = LambdaLayer(lambda x:\n","                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n","            elif option == 'B':\n","                self.shortcut = nn.Sequential(\n","                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                     nn.BatchNorm2d(self.expansion * planes)\n","                )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n","        self.linear = nn.Linear(64, num_classes)\n","\n","        self.apply(_weights_init)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.avg_pool2d(out, out.size()[3])\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","net = ResNet(BasicBlock, [3, 3, 3])\n","print(net)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): LambdaLayer()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): LambdaLayer()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=64, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fs4MD-crmsnG"},"source":["### 3. Define a loss function and optimizer"]},{"cell_type":"code","metadata":{"id":"tW5TFs-dlqdE"},"source":["import torch.optim as optim\n","\n","def createLossAndOptimizer(net, learning_rate=0.001):\n","    # it combines softmax with negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss()  \n","    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","    return criterion, optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cGLgXK93mwrl"},"source":["### 4. Train the network\n","\n","\n","This is when things start to get interesting.\n","We simply have to loop over our data iterator, feed the inputs to the network, and optimize\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VA_gXLlLmzsP"},"source":["#### Data loader"]},{"cell_type":"code","metadata":{"id":"cGTgN29Zlswa"},"source":["def get_train_loader(batch_size):\n","    return torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler,\n","                                              num_workers=num_workers)\n","\n","# Use larger batch size for validation to speed up computation\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, sampler=test_sampler,\n","                                          num_workers=num_workers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Es-EBe5Nm2g9"},"source":["#### Training loop\n","The training script: it takes ~10s per epoch with batch_size = 128"]},{"cell_type":"code","metadata":{"id":"rLdqDCIilv4t","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603007533937,"user_tz":-540,"elapsed":1149787,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"d675658b-3eab-4acd-f5f1-a77b7755b45c"},"source":["def train(net, batch_size, n_epochs, learning_rate):\n","    \"\"\"\n","    Train a neural network and print statistics of the training\n","    \n","    :param net: (PyTorch Neural Network)\n","    :param batch_size: (int)\n","    :param n_epochs: (int)  Number of iterations on the training set\n","    :param learning_rate: (float) learning rate used by the optimizer\n","    \"\"\"\n","    print(\"===== HYPERPARAMETERS =====\")\n","    print(\"batch_size=\", batch_size)\n","    print(\"n_epochs=\", n_epochs)\n","    print(\"learning_rate=\", learning_rate)\n","    print(\"=\" * 30)\n","    \n","    train_loader = get_train_loader(batch_size)\n","    n_minibatches = len(train_loader)\n","\n","    criterion, optimizer = createLossAndOptimizer(net, learning_rate)\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,40], gamma=0.1, last_epoch=-1)\n","    # Init variables used for plotting the loss\n","    train_history = []\n","    val_history = []\n","\n","    training_start_time = time.time()\n","    best_error = np.inf\n","    best_model_path = \"/content/drive/My Drive/Colab Notebooks/best_model.pth\"\n","    \n","    # Move model to gpu if possible\n","    net = net.to(device)\n","\n","    for epoch in range(n_epochs):  # loop over the dataset multiple times\n","\n","        running_loss = 0.0\n","        print_every = n_minibatches // 10\n","        start_time = time.time()\n","        total_train_loss = 0\n","        \n","        for i, (inputs, labels) in enumerate(train_loader):\n","\n","            # Move tensors to correct device\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            total_train_loss += loss.item()\n","\n","            # print every 10th of epoch\n","            if (i + 1) % (print_every + 1) == 0:    \n","                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n","                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n","                      time.time() - start_time))\n","                running_loss = 0.0\n","                start_time = time.time()\n","\n","        train_history.append(total_train_loss / len(train_loader))\n","\n","        total_val_loss = 0\n","        # Do a pass on the validation set\n","        # We don't need to compute gradient, so we use torch.no_grad() to save memory and computation\n","        with torch.no_grad():\n","          for inputs, labels in test_loader:\n","              inputs, labels = inputs.to(device), labels.to(device)\n","              predictions = net(inputs)\n","              val_loss = criterion(predictions, labels)\n","              total_val_loss += val_loss.item()\n","            \n","        val_history.append(total_val_loss / len(test_loader))\n","        # Save model that performs best on validation set\n","        if total_val_loss < best_error:\n","            best_error = total_val_loss\n","            torch.save(net.state_dict(), best_model_path)\n","\n","        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(test_loader)))\n","        scheduler.step()\n","\n","    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n","    \n","    # Load best model\n","    net.load_state_dict(torch.load(best_model_path))\n","\n","    return train_history, val_history\n","    \n","# train the model\n","train_history, val_history = train(net, batch_size=128, n_epochs=50, learning_rate=0.01)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["===== HYPERPARAMETERS =====\n","batch_size= 128\n","n_epochs= 50\n","learning_rate= 0.01\n","==============================\n","Epoch 1, 10% \t train_loss: 2.32 took: 2.25s\n","Epoch 1, 20% \t train_loss: 1.93 took: 1.95s\n","Epoch 1, 30% \t train_loss: 1.85 took: 2.04s\n","Epoch 1, 40% \t train_loss: 1.76 took: 1.96s\n","Epoch 1, 51% \t train_loss: 1.74 took: 1.92s\n","Epoch 1, 61% \t train_loss: 1.66 took: 2.00s\n","Epoch 1, 71% \t train_loss: 1.63 took: 1.98s\n","Epoch 1, 81% \t train_loss: 1.57 took: 1.98s\n","Epoch 1, 92% \t train_loss: 1.52 took: 1.97s\n","Validation loss = 1.40\n","Epoch 2, 10% \t train_loss: 1.39 took: 2.17s\n","Epoch 2, 20% \t train_loss: 1.38 took: 2.00s\n","Epoch 2, 30% \t train_loss: 1.34 took: 2.01s\n","Epoch 2, 40% \t train_loss: 1.32 took: 1.97s\n","Epoch 2, 51% \t train_loss: 1.27 took: 2.04s\n","Epoch 2, 61% \t train_loss: 1.26 took: 1.98s\n","Epoch 2, 71% \t train_loss: 1.22 took: 1.98s\n","Epoch 2, 81% \t train_loss: 1.18 took: 2.01s\n","Epoch 2, 92% \t train_loss: 1.16 took: 1.99s\n","Validation loss = 1.09\n","Epoch 3, 10% \t train_loss: 1.09 took: 2.27s\n","Epoch 3, 20% \t train_loss: 1.10 took: 2.03s\n","Epoch 3, 30% \t train_loss: 1.07 took: 2.08s\n","Epoch 3, 40% \t train_loss: 1.06 took: 2.00s\n","Epoch 3, 51% \t train_loss: 1.00 took: 2.05s\n","Epoch 3, 61% \t train_loss: 1.01 took: 2.03s\n","Epoch 3, 71% \t train_loss: 0.98 took: 2.03s\n","Epoch 3, 81% \t train_loss: 1.00 took: 2.03s\n","Epoch 3, 92% \t train_loss: 0.97 took: 2.04s\n","Validation loss = 0.93\n","Epoch 4, 10% \t train_loss: 0.92 took: 2.23s\n","Epoch 4, 20% \t train_loss: 0.94 took: 2.03s\n","Epoch 4, 30% \t train_loss: 0.92 took: 2.10s\n","Epoch 4, 40% \t train_loss: 0.89 took: 2.13s\n","Epoch 4, 51% \t train_loss: 0.89 took: 2.02s\n","Epoch 4, 61% \t train_loss: 0.86 took: 2.09s\n","Epoch 4, 71% \t train_loss: 0.85 took: 2.07s\n","Epoch 4, 81% \t train_loss: 0.85 took: 2.08s\n","Epoch 4, 92% \t train_loss: 0.85 took: 2.05s\n","Validation loss = 0.80\n","Epoch 5, 10% \t train_loss: 0.80 took: 2.30s\n","Epoch 5, 20% \t train_loss: 0.77 took: 2.05s\n","Epoch 5, 30% \t train_loss: 0.78 took: 2.05s\n","Epoch 5, 40% \t train_loss: 0.76 took: 2.04s\n","Epoch 5, 51% \t train_loss: 0.74 took: 2.13s\n","Epoch 5, 61% \t train_loss: 0.76 took: 2.06s\n","Epoch 5, 71% \t train_loss: 0.74 took: 2.12s\n","Epoch 5, 81% \t train_loss: 0.73 took: 2.02s\n","Epoch 5, 92% \t train_loss: 0.72 took: 2.13s\n","Validation loss = 0.71\n","Epoch 6, 10% \t train_loss: 0.69 took: 2.32s\n","Epoch 6, 20% \t train_loss: 0.68 took: 2.04s\n","Epoch 6, 30% \t train_loss: 0.70 took: 2.11s\n","Epoch 6, 40% \t train_loss: 0.71 took: 2.02s\n","Epoch 6, 51% \t train_loss: 0.67 took: 2.03s\n","Epoch 6, 61% \t train_loss: 0.67 took: 2.18s\n","Epoch 6, 71% \t train_loss: 0.65 took: 2.03s\n","Epoch 6, 81% \t train_loss: 0.66 took: 2.15s\n","Epoch 6, 92% \t train_loss: 0.66 took: 2.07s\n","Validation loss = 0.66\n","Epoch 7, 10% \t train_loss: 0.61 took: 2.28s\n","Epoch 7, 20% \t train_loss: 0.59 took: 2.01s\n","Epoch 7, 30% \t train_loss: 0.61 took: 2.07s\n","Epoch 7, 40% \t train_loss: 0.61 took: 2.16s\n","Epoch 7, 51% \t train_loss: 0.63 took: 2.07s\n","Epoch 7, 61% \t train_loss: 0.61 took: 2.07s\n","Epoch 7, 71% \t train_loss: 0.64 took: 2.13s\n","Epoch 7, 81% \t train_loss: 0.58 took: 2.09s\n","Epoch 7, 92% \t train_loss: 0.59 took: 2.05s\n","Validation loss = 0.61\n","Epoch 8, 10% \t train_loss: 0.58 took: 2.21s\n","Epoch 8, 20% \t train_loss: 0.58 took: 2.13s\n","Epoch 8, 30% \t train_loss: 0.55 took: 2.08s\n","Epoch 8, 40% \t train_loss: 0.54 took: 2.04s\n","Epoch 8, 51% \t train_loss: 0.57 took: 2.08s\n","Epoch 8, 61% \t train_loss: 0.58 took: 2.10s\n","Epoch 8, 71% \t train_loss: 0.54 took: 2.03s\n","Epoch 8, 81% \t train_loss: 0.56 took: 2.12s\n","Epoch 8, 92% \t train_loss: 0.57 took: 2.07s\n","Validation loss = 0.55\n","Epoch 9, 10% \t train_loss: 0.52 took: 2.27s\n","Epoch 9, 20% \t train_loss: 0.54 took: 2.08s\n","Epoch 9, 30% \t train_loss: 0.52 took: 2.10s\n","Epoch 9, 40% \t train_loss: 0.55 took: 2.04s\n","Epoch 9, 51% \t train_loss: 0.52 took: 2.10s\n","Epoch 9, 61% \t train_loss: 0.55 took: 2.10s\n","Epoch 9, 71% \t train_loss: 0.50 took: 2.05s\n","Epoch 9, 81% \t train_loss: 0.53 took: 2.09s\n","Epoch 9, 92% \t train_loss: 0.51 took: 2.08s\n","Validation loss = 0.54\n","Epoch 10, 10% \t train_loss: 0.46 took: 2.28s\n","Epoch 10, 20% \t train_loss: 0.52 took: 2.04s\n","Epoch 10, 30% \t train_loss: 0.51 took: 2.14s\n","Epoch 10, 40% \t train_loss: 0.52 took: 2.05s\n","Epoch 10, 51% \t train_loss: 0.48 took: 2.08s\n","Epoch 10, 61% \t train_loss: 0.51 took: 2.08s\n","Epoch 10, 71% \t train_loss: 0.51 took: 2.08s\n","Epoch 10, 81% \t train_loss: 0.48 took: 2.08s\n","Epoch 10, 92% \t train_loss: 0.48 took: 2.04s\n","Validation loss = 0.55\n","Epoch 11, 10% \t train_loss: 0.46 took: 2.25s\n","Epoch 11, 20% \t train_loss: 0.49 took: 2.12s\n","Epoch 11, 30% \t train_loss: 0.46 took: 2.07s\n","Epoch 11, 40% \t train_loss: 0.50 took: 2.06s\n","Epoch 11, 51% \t train_loss: 0.47 took: 2.16s\n","Epoch 11, 61% \t train_loss: 0.46 took: 2.06s\n","Epoch 11, 71% \t train_loss: 0.47 took: 2.03s\n","Epoch 11, 81% \t train_loss: 0.48 took: 2.09s\n","Epoch 11, 92% \t train_loss: 0.46 took: 2.01s\n","Validation loss = 0.50\n","Epoch 12, 10% \t train_loss: 0.42 took: 2.19s\n","Epoch 12, 20% \t train_loss: 0.43 took: 1.99s\n","Epoch 12, 30% \t train_loss: 0.45 took: 2.02s\n","Epoch 12, 40% \t train_loss: 0.45 took: 2.00s\n","Epoch 12, 51% \t train_loss: 0.46 took: 2.01s\n","Epoch 12, 61% \t train_loss: 0.47 took: 1.97s\n","Epoch 12, 71% \t train_loss: 0.46 took: 2.02s\n","Epoch 12, 81% \t train_loss: 0.45 took: 2.04s\n","Epoch 12, 92% \t train_loss: 0.45 took: 1.99s\n","Validation loss = 0.51\n","Epoch 13, 10% \t train_loss: 0.43 took: 2.15s\n","Epoch 13, 20% \t train_loss: 0.43 took: 1.99s\n","Epoch 13, 30% \t train_loss: 0.43 took: 1.99s\n","Epoch 13, 40% \t train_loss: 0.41 took: 2.02s\n","Epoch 13, 51% \t train_loss: 0.43 took: 1.99s\n","Epoch 13, 61% \t train_loss: 0.42 took: 2.00s\n","Epoch 13, 71% \t train_loss: 0.40 took: 1.98s\n","Epoch 13, 81% \t train_loss: 0.42 took: 1.99s\n","Epoch 13, 92% \t train_loss: 0.46 took: 1.96s\n","Validation loss = 0.47\n","Epoch 14, 10% \t train_loss: 0.40 took: 2.19s\n","Epoch 14, 20% \t train_loss: 0.41 took: 2.04s\n","Epoch 14, 30% \t train_loss: 0.40 took: 1.99s\n","Epoch 14, 40% \t train_loss: 0.41 took: 2.00s\n","Epoch 14, 51% \t train_loss: 0.43 took: 1.98s\n","Epoch 14, 61% \t train_loss: 0.43 took: 2.00s\n","Epoch 14, 71% \t train_loss: 0.38 took: 1.99s\n","Epoch 14, 81% \t train_loss: 0.40 took: 2.01s\n","Epoch 14, 92% \t train_loss: 0.42 took: 2.01s\n","Validation loss = 0.45\n","Epoch 15, 10% \t train_loss: 0.38 took: 2.19s\n","Epoch 15, 20% \t train_loss: 0.38 took: 2.00s\n","Epoch 15, 30% \t train_loss: 0.41 took: 1.98s\n","Epoch 15, 40% \t train_loss: 0.38 took: 1.99s\n","Epoch 15, 51% \t train_loss: 0.40 took: 2.01s\n","Epoch 15, 61% \t train_loss: 0.38 took: 2.05s\n","Epoch 15, 71% \t train_loss: 0.40 took: 1.98s\n","Epoch 15, 81% \t train_loss: 0.39 took: 2.03s\n","Epoch 15, 92% \t train_loss: 0.39 took: 2.00s\n","Validation loss = 0.46\n","Epoch 16, 10% \t train_loss: 0.33 took: 2.16s\n","Epoch 16, 20% \t train_loss: 0.37 took: 2.03s\n","Epoch 16, 30% \t train_loss: 0.38 took: 1.96s\n","Epoch 16, 40% \t train_loss: 0.38 took: 1.99s\n","Epoch 16, 51% \t train_loss: 0.37 took: 1.97s\n","Epoch 16, 61% \t train_loss: 0.38 took: 1.96s\n","Epoch 16, 71% \t train_loss: 0.35 took: 2.01s\n","Epoch 16, 81% \t train_loss: 0.35 took: 2.01s\n","Epoch 16, 92% \t train_loss: 0.41 took: 1.98s\n","Validation loss = 0.45\n","Epoch 17, 10% \t train_loss: 0.36 took: 2.17s\n","Epoch 17, 20% \t train_loss: 0.35 took: 2.03s\n","Epoch 17, 30% \t train_loss: 0.35 took: 2.04s\n","Epoch 17, 40% \t train_loss: 0.34 took: 1.97s\n","Epoch 17, 51% \t train_loss: 0.37 took: 1.99s\n","Epoch 17, 61% \t train_loss: 0.39 took: 2.04s\n","Epoch 17, 71% \t train_loss: 0.36 took: 2.02s\n","Epoch 17, 81% \t train_loss: 0.37 took: 2.03s\n","Epoch 17, 92% \t train_loss: 0.35 took: 2.04s\n","Validation loss = 0.45\n","Epoch 18, 10% \t train_loss: 0.33 took: 2.18s\n","Epoch 18, 20% \t train_loss: 0.35 took: 2.00s\n","Epoch 18, 30% \t train_loss: 0.33 took: 1.98s\n","Epoch 18, 40% \t train_loss: 0.36 took: 1.99s\n","Epoch 18, 51% \t train_loss: 0.35 took: 2.01s\n","Epoch 18, 61% \t train_loss: 0.37 took: 2.00s\n","Epoch 18, 71% \t train_loss: 0.37 took: 1.97s\n","Epoch 18, 81% \t train_loss: 0.37 took: 1.99s\n","Epoch 18, 92% \t train_loss: 0.31 took: 1.97s\n","Validation loss = 0.46\n","Epoch 19, 10% \t train_loss: 0.32 took: 2.16s\n","Epoch 19, 20% \t train_loss: 0.32 took: 2.00s\n","Epoch 19, 30% \t train_loss: 0.34 took: 1.98s\n","Epoch 19, 40% \t train_loss: 0.32 took: 2.01s\n","Epoch 19, 51% \t train_loss: 0.33 took: 1.98s\n","Epoch 19, 61% \t train_loss: 0.34 took: 1.97s\n","Epoch 19, 71% \t train_loss: 0.33 took: 2.05s\n","Epoch 19, 81% \t train_loss: 0.36 took: 1.99s\n","Epoch 19, 92% \t train_loss: 0.36 took: 2.01s\n","Validation loss = 0.43\n","Epoch 20, 10% \t train_loss: 0.30 took: 2.14s\n","Epoch 20, 20% \t train_loss: 0.33 took: 2.01s\n","Epoch 20, 30% \t train_loss: 0.34 took: 1.98s\n","Epoch 20, 40% \t train_loss: 0.32 took: 2.02s\n","Epoch 20, 51% \t train_loss: 0.32 took: 2.01s\n","Epoch 20, 61% \t train_loss: 0.31 took: 2.01s\n","Epoch 20, 71% \t train_loss: 0.31 took: 1.99s\n","Epoch 20, 81% \t train_loss: 0.34 took: 2.02s\n","Epoch 20, 92% \t train_loss: 0.36 took: 1.94s\n","Validation loss = 0.43\n","Epoch 21, 10% \t train_loss: 0.31 took: 2.20s\n","Epoch 21, 20% \t train_loss: 0.29 took: 1.99s\n","Epoch 21, 30% \t train_loss: 0.32 took: 1.97s\n","Epoch 21, 40% \t train_loss: 0.29 took: 2.01s\n","Epoch 21, 51% \t train_loss: 0.30 took: 2.00s\n","Epoch 21, 61% \t train_loss: 0.32 took: 1.99s\n","Epoch 21, 71% \t train_loss: 0.32 took: 1.97s\n","Epoch 21, 81% \t train_loss: 0.36 took: 2.00s\n","Epoch 21, 92% \t train_loss: 0.32 took: 1.99s\n","Validation loss = 0.43\n","Epoch 22, 10% \t train_loss: 0.29 took: 2.19s\n","Epoch 22, 20% \t train_loss: 0.30 took: 1.96s\n","Epoch 22, 30% \t train_loss: 0.30 took: 1.97s\n","Epoch 22, 40% \t train_loss: 0.32 took: 2.02s\n","Epoch 22, 51% \t train_loss: 0.29 took: 1.99s\n","Epoch 22, 61% \t train_loss: 0.29 took: 2.01s\n","Epoch 22, 71% \t train_loss: 0.32 took: 2.02s\n","Epoch 22, 81% \t train_loss: 0.32 took: 2.01s\n","Epoch 22, 92% \t train_loss: 0.32 took: 1.97s\n","Validation loss = 0.40\n","Epoch 23, 10% \t train_loss: 0.29 took: 2.17s\n","Epoch 23, 20% \t train_loss: 0.29 took: 2.00s\n","Epoch 23, 30% \t train_loss: 0.30 took: 1.99s\n","Epoch 23, 40% \t train_loss: 0.30 took: 2.04s\n","Epoch 23, 51% \t train_loss: 0.30 took: 1.95s\n","Epoch 23, 61% \t train_loss: 0.30 took: 2.05s\n","Epoch 23, 71% \t train_loss: 0.32 took: 1.97s\n","Epoch 23, 81% \t train_loss: 0.27 took: 1.99s\n","Epoch 23, 92% \t train_loss: 0.31 took: 2.02s\n","Validation loss = 0.42\n","Epoch 24, 10% \t train_loss: 0.26 took: 2.21s\n","Epoch 24, 20% \t train_loss: 0.30 took: 1.99s\n","Epoch 24, 30% \t train_loss: 0.28 took: 1.98s\n","Epoch 24, 40% \t train_loss: 0.29 took: 1.96s\n","Epoch 24, 51% \t train_loss: 0.28 took: 2.00s\n","Epoch 24, 61% \t train_loss: 0.28 took: 1.98s\n","Epoch 24, 71% \t train_loss: 0.29 took: 2.00s\n","Epoch 24, 81% \t train_loss: 0.30 took: 2.00s\n","Epoch 24, 92% \t train_loss: 0.30 took: 1.99s\n","Validation loss = 0.41\n","Epoch 25, 10% \t train_loss: 0.27 took: 2.19s\n","Epoch 25, 20% \t train_loss: 0.26 took: 1.99s\n","Epoch 25, 30% \t train_loss: 0.27 took: 1.98s\n","Epoch 25, 40% \t train_loss: 0.28 took: 2.03s\n","Epoch 25, 51% \t train_loss: 0.27 took: 1.95s\n","Epoch 25, 61% \t train_loss: 0.29 took: 2.00s\n","Epoch 25, 71% \t train_loss: 0.27 took: 2.06s\n","Epoch 25, 81% \t train_loss: 0.28 took: 1.97s\n","Epoch 25, 92% \t train_loss: 0.28 took: 1.96s\n","Validation loss = 0.41\n","Epoch 26, 10% \t train_loss: 0.26 took: 2.17s\n","Epoch 26, 20% \t train_loss: 0.25 took: 1.98s\n","Epoch 26, 30% \t train_loss: 0.27 took: 2.00s\n","Epoch 26, 40% \t train_loss: 0.27 took: 1.99s\n","Epoch 26, 51% \t train_loss: 0.28 took: 1.93s\n","Epoch 26, 61% \t train_loss: 0.29 took: 2.01s\n","Epoch 26, 71% \t train_loss: 0.26 took: 1.96s\n","Epoch 26, 81% \t train_loss: 0.26 took: 2.03s\n","Epoch 26, 92% \t train_loss: 0.28 took: 2.01s\n","Validation loss = 0.42\n","Epoch 27, 10% \t train_loss: 0.26 took: 2.20s\n","Epoch 27, 20% \t train_loss: 0.28 took: 2.00s\n","Epoch 27, 30% \t train_loss: 0.26 took: 1.95s\n","Epoch 27, 40% \t train_loss: 0.26 took: 2.02s\n","Epoch 27, 51% \t train_loss: 0.28 took: 1.97s\n","Epoch 27, 61% \t train_loss: 0.28 took: 2.01s\n","Epoch 27, 71% \t train_loss: 0.28 took: 1.99s\n","Epoch 27, 81% \t train_loss: 0.28 took: 2.04s\n","Epoch 27, 92% \t train_loss: 0.28 took: 1.92s\n","Validation loss = 0.41\n","Epoch 28, 10% \t train_loss: 0.26 took: 2.17s\n","Epoch 28, 20% \t train_loss: 0.23 took: 2.04s\n","Epoch 28, 30% \t train_loss: 0.22 took: 2.04s\n","Epoch 28, 40% \t train_loss: 0.26 took: 1.96s\n","Epoch 28, 51% \t train_loss: 0.27 took: 1.94s\n","Epoch 28, 61% \t train_loss: 0.26 took: 2.09s\n","Epoch 28, 71% \t train_loss: 0.25 took: 1.97s\n","Epoch 28, 81% \t train_loss: 0.28 took: 2.00s\n","Epoch 28, 92% \t train_loss: 0.26 took: 1.94s\n","Validation loss = 0.41\n","Epoch 29, 10% \t train_loss: 0.25 took: 2.11s\n","Epoch 29, 20% \t train_loss: 0.24 took: 2.01s\n","Epoch 29, 30% \t train_loss: 0.27 took: 2.06s\n","Epoch 29, 40% \t train_loss: 0.26 took: 2.00s\n","Epoch 29, 51% \t train_loss: 0.24 took: 2.06s\n","Epoch 29, 61% \t train_loss: 0.24 took: 2.09s\n","Epoch 29, 71% \t train_loss: 0.25 took: 2.02s\n","Epoch 29, 81% \t train_loss: 0.25 took: 2.02s\n","Epoch 29, 92% \t train_loss: 0.24 took: 1.99s\n","Validation loss = 0.42\n","Epoch 30, 10% \t train_loss: 0.23 took: 2.22s\n","Epoch 30, 20% \t train_loss: 0.22 took: 2.03s\n","Epoch 30, 30% \t train_loss: 0.22 took: 2.10s\n","Epoch 30, 40% \t train_loss: 0.26 took: 1.96s\n","Epoch 30, 51% \t train_loss: 0.23 took: 2.06s\n","Epoch 30, 61% \t train_loss: 0.25 took: 1.97s\n","Epoch 30, 71% \t train_loss: 0.24 took: 1.93s\n","Epoch 30, 81% \t train_loss: 0.26 took: 2.06s\n","Epoch 30, 92% \t train_loss: 0.24 took: 1.95s\n","Validation loss = 0.42\n","Epoch 31, 10% \t train_loss: 0.23 took: 2.18s\n","Epoch 31, 20% \t train_loss: 0.19 took: 1.99s\n","Epoch 31, 30% \t train_loss: 0.19 took: 2.03s\n","Epoch 31, 40% \t train_loss: 0.18 took: 2.09s\n","Epoch 31, 51% \t train_loss: 0.16 took: 2.02s\n","Epoch 31, 61% \t train_loss: 0.18 took: 1.96s\n","Epoch 31, 71% \t train_loss: 0.17 took: 2.08s\n","Epoch 31, 81% \t train_loss: 0.17 took: 2.00s\n","Epoch 31, 92% \t train_loss: 0.18 took: 2.01s\n","Validation loss = 0.35\n","Epoch 32, 10% \t train_loss: 0.17 took: 2.22s\n","Epoch 32, 20% \t train_loss: 0.16 took: 1.96s\n","Epoch 32, 30% \t train_loss: 0.15 took: 2.04s\n","Epoch 32, 40% \t train_loss: 0.17 took: 2.02s\n","Epoch 32, 51% \t train_loss: 0.16 took: 1.98s\n","Epoch 32, 61% \t train_loss: 0.16 took: 2.01s\n","Epoch 32, 71% \t train_loss: 0.15 took: 2.01s\n","Epoch 32, 81% \t train_loss: 0.16 took: 1.93s\n","Epoch 32, 92% \t train_loss: 0.15 took: 1.99s\n","Validation loss = 0.35\n","Epoch 33, 10% \t train_loss: 0.15 took: 2.18s\n","Epoch 33, 20% \t train_loss: 0.14 took: 1.96s\n","Epoch 33, 30% \t train_loss: 0.15 took: 1.97s\n","Epoch 33, 40% \t train_loss: 0.15 took: 2.00s\n","Epoch 33, 51% \t train_loss: 0.15 took: 1.96s\n","Epoch 33, 61% \t train_loss: 0.15 took: 2.00s\n","Epoch 33, 71% \t train_loss: 0.14 took: 1.99s\n","Epoch 33, 81% \t train_loss: 0.15 took: 1.97s\n","Epoch 33, 92% \t train_loss: 0.14 took: 2.00s\n","Validation loss = 0.34\n","Epoch 34, 10% \t train_loss: 0.15 took: 2.20s\n","Epoch 34, 20% \t train_loss: 0.14 took: 2.02s\n","Epoch 34, 30% \t train_loss: 0.14 took: 1.99s\n","Epoch 34, 40% \t train_loss: 0.15 took: 1.99s\n","Epoch 34, 51% \t train_loss: 0.14 took: 2.00s\n","Epoch 34, 61% \t train_loss: 0.13 took: 2.00s\n","Epoch 34, 71% \t train_loss: 0.14 took: 2.01s\n","Epoch 34, 81% \t train_loss: 0.14 took: 1.96s\n","Epoch 34, 92% \t train_loss: 0.14 took: 2.02s\n","Validation loss = 0.37\n","Epoch 35, 10% \t train_loss: 0.14 took: 2.19s\n","Epoch 35, 20% \t train_loss: 0.13 took: 2.00s\n","Epoch 35, 30% \t train_loss: 0.14 took: 2.01s\n","Epoch 35, 40% \t train_loss: 0.14 took: 1.95s\n","Epoch 35, 51% \t train_loss: 0.13 took: 1.99s\n","Epoch 35, 61% \t train_loss: 0.13 took: 1.98s\n","Epoch 35, 71% \t train_loss: 0.14 took: 1.98s\n","Epoch 35, 81% \t train_loss: 0.13 took: 2.02s\n","Epoch 35, 92% \t train_loss: 0.15 took: 1.98s\n","Validation loss = 0.36\n","Epoch 36, 10% \t train_loss: 0.13 took: 2.21s\n","Epoch 36, 20% \t train_loss: 0.13 took: 2.00s\n","Epoch 36, 30% \t train_loss: 0.13 took: 1.98s\n","Epoch 36, 40% \t train_loss: 0.13 took: 1.92s\n","Epoch 36, 51% \t train_loss: 0.13 took: 2.02s\n","Epoch 36, 61% \t train_loss: 0.13 took: 2.00s\n","Epoch 36, 71% \t train_loss: 0.12 took: 1.99s\n","Epoch 36, 81% \t train_loss: 0.13 took: 1.95s\n","Epoch 36, 92% \t train_loss: 0.14 took: 1.97s\n","Validation loss = 0.35\n","Epoch 37, 10% \t train_loss: 0.13 took: 2.18s\n","Epoch 37, 20% \t train_loss: 0.13 took: 1.93s\n","Epoch 37, 30% \t train_loss: 0.13 took: 1.96s\n","Epoch 37, 40% \t train_loss: 0.13 took: 1.95s\n","Epoch 37, 51% \t train_loss: 0.13 took: 1.92s\n","Epoch 37, 61% \t train_loss: 0.14 took: 1.98s\n","Epoch 37, 71% \t train_loss: 0.12 took: 1.96s\n","Epoch 37, 81% \t train_loss: 0.13 took: 2.00s\n","Epoch 37, 92% \t train_loss: 0.12 took: 2.00s\n","Validation loss = 0.35\n","Epoch 38, 10% \t train_loss: 0.11 took: 2.16s\n","Epoch 38, 20% \t train_loss: 0.13 took: 1.99s\n","Epoch 38, 30% \t train_loss: 0.12 took: 1.97s\n","Epoch 38, 40% \t train_loss: 0.13 took: 1.98s\n","Epoch 38, 51% \t train_loss: 0.12 took: 2.00s\n","Epoch 38, 61% \t train_loss: 0.13 took: 1.97s\n","Epoch 38, 71% \t train_loss: 0.13 took: 1.93s\n","Epoch 38, 81% \t train_loss: 0.13 took: 1.97s\n","Epoch 38, 92% \t train_loss: 0.12 took: 1.99s\n","Validation loss = 0.37\n","Epoch 39, 10% \t train_loss: 0.11 took: 2.17s\n","Epoch 39, 20% \t train_loss: 0.12 took: 1.98s\n","Epoch 39, 30% \t train_loss: 0.12 took: 2.01s\n","Epoch 39, 40% \t train_loss: 0.13 took: 1.99s\n","Epoch 39, 51% \t train_loss: 0.12 took: 1.97s\n","Epoch 39, 61% \t train_loss: 0.12 took: 1.96s\n","Epoch 39, 71% \t train_loss: 0.11 took: 1.95s\n","Epoch 39, 81% \t train_loss: 0.14 took: 1.97s\n","Epoch 39, 92% \t train_loss: 0.13 took: 1.97s\n","Validation loss = 0.36\n","Epoch 40, 10% \t train_loss: 0.12 took: 2.14s\n","Epoch 40, 20% \t train_loss: 0.11 took: 1.95s\n","Epoch 40, 30% \t train_loss: 0.12 took: 1.99s\n","Epoch 40, 40% \t train_loss: 0.12 took: 1.95s\n","Epoch 40, 51% \t train_loss: 0.12 took: 2.04s\n","Epoch 40, 61% \t train_loss: 0.12 took: 1.96s\n","Epoch 40, 71% \t train_loss: 0.12 took: 1.98s\n","Epoch 40, 81% \t train_loss: 0.12 took: 1.98s\n","Epoch 40, 92% \t train_loss: 0.12 took: 1.94s\n","Validation loss = 0.36\n","Epoch 41, 10% \t train_loss: 0.11 took: 2.16s\n","Epoch 41, 20% \t train_loss: 0.11 took: 1.99s\n","Epoch 41, 30% \t train_loss: 0.11 took: 1.91s\n","Epoch 41, 40% \t train_loss: 0.12 took: 1.98s\n","Epoch 41, 51% \t train_loss: 0.12 took: 1.94s\n","Epoch 41, 61% \t train_loss: 0.11 took: 2.02s\n","Epoch 41, 71% \t train_loss: 0.11 took: 1.99s\n","Epoch 41, 81% \t train_loss: 0.11 took: 2.01s\n","Epoch 41, 92% \t train_loss: 0.11 took: 2.02s\n","Validation loss = 0.36\n","Epoch 42, 10% \t train_loss: 0.10 took: 2.19s\n","Epoch 42, 20% \t train_loss: 0.11 took: 2.09s\n","Epoch 42, 30% \t train_loss: 0.11 took: 1.99s\n","Epoch 42, 40% \t train_loss: 0.11 took: 2.05s\n","Epoch 42, 51% \t train_loss: 0.11 took: 2.08s\n","Epoch 42, 61% \t train_loss: 0.11 took: 2.00s\n","Epoch 42, 71% \t train_loss: 0.11 took: 2.02s\n","Epoch 42, 81% \t train_loss: 0.11 took: 1.96s\n","Epoch 42, 92% \t train_loss: 0.11 took: 2.04s\n","Validation loss = 0.36\n","Epoch 43, 10% \t train_loss: 0.11 took: 2.17s\n","Epoch 43, 20% \t train_loss: 0.11 took: 2.02s\n","Epoch 43, 30% \t train_loss: 0.11 took: 2.01s\n","Epoch 43, 40% \t train_loss: 0.12 took: 2.03s\n","Epoch 43, 51% \t train_loss: 0.11 took: 2.01s\n","Epoch 43, 61% \t train_loss: 0.12 took: 1.97s\n","Epoch 43, 71% \t train_loss: 0.11 took: 2.00s\n","Epoch 43, 81% \t train_loss: 0.12 took: 2.03s\n","Epoch 43, 92% \t train_loss: 0.10 took: 1.98s\n","Validation loss = 0.38\n","Epoch 44, 10% \t train_loss: 0.12 took: 2.19s\n","Epoch 44, 20% \t train_loss: 0.11 took: 2.02s\n","Epoch 44, 30% \t train_loss: 0.09 took: 1.99s\n","Epoch 44, 40% \t train_loss: 0.11 took: 1.98s\n","Epoch 44, 51% \t train_loss: 0.11 took: 1.97s\n","Epoch 44, 61% \t train_loss: 0.10 took: 2.03s\n","Epoch 44, 71% \t train_loss: 0.11 took: 2.04s\n","Epoch 44, 81% \t train_loss: 0.12 took: 2.02s\n","Epoch 44, 92% \t train_loss: 0.11 took: 2.00s\n","Validation loss = 0.37\n","Epoch 45, 10% \t train_loss: 0.11 took: 2.21s\n","Epoch 45, 20% \t train_loss: 0.11 took: 2.03s\n","Epoch 45, 30% \t train_loss: 0.10 took: 2.04s\n","Epoch 45, 40% \t train_loss: 0.10 took: 2.02s\n","Epoch 45, 51% \t train_loss: 0.11 took: 2.05s\n","Epoch 45, 61% \t train_loss: 0.10 took: 1.95s\n","Epoch 45, 71% \t train_loss: 0.10 took: 2.06s\n","Epoch 45, 81% \t train_loss: 0.10 took: 2.00s\n","Epoch 45, 92% \t train_loss: 0.10 took: 1.97s\n","Validation loss = 0.37\n","Epoch 46, 10% \t train_loss: 0.11 took: 2.20s\n","Epoch 46, 20% \t train_loss: 0.11 took: 1.96s\n","Epoch 46, 30% \t train_loss: 0.11 took: 2.03s\n","Epoch 46, 40% \t train_loss: 0.10 took: 1.98s\n","Epoch 46, 51% \t train_loss: 0.12 took: 1.99s\n","Epoch 46, 61% \t train_loss: 0.09 took: 2.06s\n","Epoch 46, 71% \t train_loss: 0.11 took: 1.96s\n","Epoch 46, 81% \t train_loss: 0.11 took: 2.01s\n","Epoch 46, 92% \t train_loss: 0.11 took: 1.99s\n","Validation loss = 0.37\n","Epoch 47, 10% \t train_loss: 0.11 took: 2.16s\n","Epoch 47, 20% \t train_loss: 0.11 took: 2.05s\n","Epoch 47, 30% \t train_loss: 0.10 took: 1.95s\n","Epoch 47, 40% \t train_loss: 0.11 took: 2.06s\n","Epoch 47, 51% \t train_loss: 0.11 took: 2.02s\n","Epoch 47, 61% \t train_loss: 0.10 took: 2.01s\n","Epoch 47, 71% \t train_loss: 0.10 took: 1.99s\n","Epoch 47, 81% \t train_loss: 0.10 took: 2.01s\n","Epoch 47, 92% \t train_loss: 0.11 took: 2.01s\n","Validation loss = 0.35\n","Epoch 48, 10% \t train_loss: 0.11 took: 2.18s\n","Epoch 48, 20% \t train_loss: 0.11 took: 2.02s\n","Epoch 48, 30% \t train_loss: 0.11 took: 2.02s\n","Epoch 48, 40% \t train_loss: 0.11 took: 1.99s\n","Epoch 48, 51% \t train_loss: 0.10 took: 1.97s\n","Epoch 48, 61% \t train_loss: 0.10 took: 2.01s\n","Epoch 48, 71% \t train_loss: 0.11 took: 2.02s\n","Epoch 48, 81% \t train_loss: 0.10 took: 2.01s\n","Epoch 48, 92% \t train_loss: 0.10 took: 2.00s\n","Validation loss = 0.38\n","Epoch 49, 10% \t train_loss: 0.10 took: 2.15s\n","Epoch 49, 20% \t train_loss: 0.10 took: 1.97s\n","Epoch 49, 30% \t train_loss: 0.09 took: 1.95s\n","Epoch 49, 40% \t train_loss: 0.11 took: 1.95s\n","Epoch 49, 51% \t train_loss: 0.11 took: 1.97s\n","Epoch 49, 61% \t train_loss: 0.11 took: 1.97s\n","Epoch 49, 71% \t train_loss: 0.10 took: 2.03s\n","Epoch 49, 81% \t train_loss: 0.10 took: 2.06s\n","Epoch 49, 92% \t train_loss: 0.11 took: 2.01s\n","Validation loss = 0.36\n","Epoch 50, 10% \t train_loss: 0.09 took: 2.19s\n","Epoch 50, 20% \t train_loss: 0.10 took: 2.00s\n","Epoch 50, 30% \t train_loss: 0.11 took: 1.96s\n","Epoch 50, 40% \t train_loss: 0.11 took: 2.01s\n","Epoch 50, 51% \t train_loss: 0.11 took: 1.98s\n","Epoch 50, 61% \t train_loss: 0.11 took: 2.00s\n","Epoch 50, 71% \t train_loss: 0.10 took: 2.03s\n","Epoch 50, 81% \t train_loss: 0.11 took: 1.98s\n","Epoch 50, 92% \t train_loss: 0.10 took: 1.99s\n","Validation loss = 0.37\n","Training Finished, took 1145.11s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_WXsVke3m8gP"},"source":["Now, let's look at the plot of the training loss and validation loss as a function of the number of epochs."]},{"cell_type":"code","metadata":{"id":"BbAHpNDHm9yP","colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"status":"ok","timestamp":1603007533941,"user_tz":-540,"elapsed":1149782,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"759029cd-5a9e-45d7-f8b8-52726f9e50a8"},"source":["plot_losses(train_history, val_history)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x432 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"L1u7FODPnFgj"},"source":["5. Test the network on the test data"]},{"cell_type":"markdown","metadata":{"id":"9nK5SDe_nRry"},"source":["The outputs are energies for the 10 classes.\n","The higher the energy for a class, the more the network\n","thinks that the image is from that particular class.\n","So, let's get the index of the highest energy:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Taf_-SSj1Dc_"},"source":["Finally, let us look at how the network performs on the whole test set."]},{"cell_type":"code","metadata":{"id":"-rGoEl_cnZBn","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603009525413,"user_tz":-540,"elapsed":80644,"user":{"displayName":"Daniel Gwak","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4FSGzeRk6uQDHtNktqDTV4A8o30n0ewJ7gOlB=s64","userId":"13581691481666858069"}},"outputId":"962bf922-14cd-4bc6-f749-b1ae3e0c39e3"},"source":["def dataset_accuracy(net, data_loader, name=\"\"):\n","    net.eval()\n","    net = net.to(device)\n","    correct = 0\n","    total = 0\n","    for images, labels in data_loader:\n","      images, labels = images.to(device), labels.to(device)\n","      outputs = net(images)\n","      _,predictions = torch.max(outputs, 1)\n","      total += labels.size(0)\n","      correct += (predictions == labels).sum()\n","      \n","    accuracy = 100 * float(correct) / total\n","    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n","\n","def train_set_accuracy(net):\n","    dataset_accuracy(net, train_loader, \"train\")\n","\n","def test_set_accuracy(net):\n","    dataset_accuracy(net, test_loader, \"test\")\n","\n","def compute_accuracy(net):\n","    train_set_accuracy(net)\n","    test_set_accuracy(net)\n","    \n","print(\"Computing accuracy...\")\n","compute_accuracy(net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Computing accuracy...\n","Accuracy of the network on the 50000 train images: 95.67 %\n","Accuracy of the network on the 10000 test images: 89.82 %\n"],"name":"stdout"}]}]}